{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is scikit-learn\n",
    "\n",
    "Scikit learn provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib.\n",
    "\n",
    "Scikit-learn includes following features:\n",
    "\n",
    "\n",
    "1. Supervised Learning algorithms − Almost all the popular supervised learning algorithms, like Linear Regression, Support Vector Machine (SVM), Decision Tree etc., are the part of scikit-learn.\n",
    "\n",
    "2. Unsupervised Learning algorithms − On the other hand, it also has all the popular unsupervised learning algorithms from clustering, factor analysis, PCA (Principal Component Analysis) to unsupervised neural networks.\n",
    "\n",
    "3. Clustering − This model is used for grouping unlabeled data.\n",
    "\n",
    "4. Cross Validation − It is used to check the accuracy of supervised models on unseen data.\n",
    "\n",
    "5. Dimensionality Reduction − It is used for reducing the number of attributes in data which can be further used for summarisation, visualisation and feature selection.\n",
    "\n",
    "6. Ensemble methods − As name suggest, it is used for combining the predictions of multiple supervised models.\n",
    "\n",
    "7. Feature extraction − It is used to extract the features from data to define the attributes in image and text data.\n",
    "\n",
    "8. Feature selection − It is used to identify useful attributes to create supervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set loading\n",
    "Scikit-learn have few example datasets like <strong>iris</strong> and <strong>digits</strong> for classification and the <strong>Boston house prices</strong> for regression.\n",
    "\n",
    "The following code shows an example of loading iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "First 10 rows of X:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "print(\"\\nFirst 10 rows of X:\\n\", X[:10])\n",
    "# As you can see, the data in the iris dataset is recorded using numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test and train sets\n",
    "We can split the dataset for training and testing. The following example will split the data into 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, trasin_label, test_label  = train_test_split(X, y , train_size=0.7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "105\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "Next, we can use our dataset to train some prediction-model. There is a wide range of Machine Learning (ML) algorithms. For example, we use KNN for classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "knn_classifier = KNeighborsClassifier(3)\n",
    "knn_classifier.fit(train_data,trasin_label)\n",
    "result = knn_classifier.predict(test_data)\n",
    "accuracy = metrics.accuracy_score(test_label , result)\n",
    "print(\"Accuracy =\" , accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence\n",
    "Once you train the model, it is desirable that the model should be persist for future use so that we do not need to retrain it again and again. It can be done with the help of dump and load features of joblib package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_classifier.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(knn_classifier, \"knn_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can load the saved model using the following method:\n",
    "loaded_classifier = joblib.load(\"knn_classifier.joblib\")\n",
    "loaded_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "Scikit-learn has package named preprocessing for this purpose. The preprocessing package has the following techniques:\n",
    "1. Binarisation\n",
    "2. Mean removal\n",
    "3. Scaling\n",
    "4. Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 1., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarisation: This preprocessing technique is used when we need to convert our numerical values into Boolean values.\n",
    "from sklearn import preprocessing\n",
    "data = [[2.1, -1.9, 5.5],\n",
    "   [-1.5, 2.4, 3.5],\n",
    "   [0.5, -7.9, 5.6],\n",
    "   [5.9, 2.3, -5.8]]\n",
    "preprocessing.binarize(data,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean =  [ 1.75  -1.275  2.2  ]\n",
      "Std =  [2.71431391 4.20022321 4.69414529]\n",
      "[[ 0.12894603 -0.14880162  0.70300338]\n",
      " [-1.19735598  0.8749535   0.27694073]\n",
      " [-0.46052153 -1.57729713  0.72430651]\n",
      " [ 1.52893149  0.85114524 -1.70425062]]\n",
      "Mean after scale =  [1.11022302e-16 0.00000000e+00 0.00000000e+00]\n",
      "Std after scale =  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Mean removal: This technique is used to eliminate the mean from feature vector so that every feature centered on zero.\n",
    "import numpy as np\n",
    "print(\"Mean = \", np.mean(data,axis=0))\n",
    "print(\"Std = \" , np.std(data,axis=0))\n",
    "\n",
    "data_zero_centered = preprocessing.scale(data)\n",
    "\n",
    "print(data_zero_centered)\n",
    "print(\"Mean after scale = \", np.mean(data_zero_centered,axis=0))\n",
    "print(\"Std after scale = \", np.std(data_zero_centered,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02702703  0.16504854  0.98245614]\n",
      " [-1.          1.          0.63157895]\n",
      " [-0.45945946 -1.          1.        ]\n",
      " [ 1.          0.98058252 -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#  Scaling: We use this preprocessing technique for scaling the feature vectors.\n",
    "#  Scaling of feature vectors is important, because the features should not be synthetically large or small.\n",
    "min_max = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "data_scaled = min_max.fit_transform(data_zero_centered)\n",
    "print(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0230109   0.14052285  0.83646625]\n",
      " [-0.38        0.38        0.24      ]\n",
      " [-0.18681319 -0.40659341  0.40659341]\n",
      " [ 0.33550489  0.32899023 -0.33550489]]\n",
      "\n",
      "[[-0.02711951  0.16561329  0.98581782]\n",
      " [-0.64564628  0.64564628  0.4077766 ]\n",
      " [-0.30898878 -0.67250499  0.67250499]\n",
      " [ 0.58108685  0.56980361 -0.58108685]]\n"
     ]
    }
   ],
   "source": [
    "# We use this preprocessing technique for modifying the feature vectors.\n",
    "# Normalisation of feature vectors is necessary so that the feature vectors can be measured at common scale\n",
    "\n",
    "# L1 normalization: the sum of the absolute values remains always up to 1 in each row.\n",
    "l1_data = preprocessing.normalize(data_scaled,\"l1\")\n",
    "print(l1_data)\n",
    "print()\n",
    "\n",
    "# L2 normalization: the sum of the squares remains always up to 1 in each row.\n",
    "l2_data = preprocessing.normalize(data_scaled,\"l2\")\n",
    "print(l2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator API\n",
    "\n",
    "All machine learning algorithms in Scikit-Learn are implemented via Estimator API. The object that learns from the data (fitting the data) is an estimator. It can be used with any of the algorithms like classification, regression, clustering or even with a transformer, that extracts useful features from raw data.\n",
    "\n",
    "Steps in using Estimator API:\n",
    "\n",
    "Step 1: Choose a class of model\n",
    "In this first step, we need to choose a class of model. It can be done by importing the appropriate Estimator class from Scikit-learn.\n",
    "\n",
    "Step 2: Choose model hyperparameters\n",
    "In this step, we need to choose class model hyperparameters. It can be done by instantiating the class with desired values.\n",
    "\n",
    "Step 3: Arranging the data\n",
    "Next, we need to arrange the data into features matrix (X) and target vector(y).\n",
    "\n",
    "Step 4: Model Fitting\n",
    "Now, we need to fit the model to your data. It can be done by calling fit() method of the model instance.\n",
    "\n",
    "Step 5: Applying the model\n",
    "After fitting the model, we can apply it to new data. For supervised learning, use predict() method to predict the labels for unknown data. While for unsupervised learning, use predict() or transform() to infer properties of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Modeling with scikit-learn\n",
    "\n",
    "A linear model is an equation that describes a relationship between two quantities that show a constant rate of change. We represent linear relationships graphically with straight lines. A linear model is usually described by two parameters: the slope, often called the growth factor or rate of change, and the y-intercept, often called the initial value. Given the slope mm and the yy-intercept b,b, the linear model can be written as a linear function y = mx + b.y=mx+b. For example, W\n",
    "we can represent the position of a car moving at a constant velocity with a linear model.\n",
    "\n",
    "The following list shows the various linear models provided by Scikit-Learn:\n",
    "\n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Ridge Regression\n",
    "4. Bayesian Ridge Regression\n",
    "5. LASSO\n",
    "6. Multi-task LASSO\n",
    "7. Elastic-Net\n",
    "8. Multi-task Elastic-Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Linear regression is a statistical model that studies the relationship between a dependent variable (Y) with a given set of independent variables (X). \n",
    "<code>sklearn.linear_model.LinearRegression</code> is the module used to implement linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[1 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [2 3]]\n",
      "y =  [ 6  8  9 11]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression(n_jobs=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression(n_jobs=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression(n_jobs=2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array([[1,1],[1,2],[2,2],[2,3]])\n",
    "print(\"X = \" , X)\n",
    "y = np.dot(X,[1,2]) + 3\n",
    "print(\"y = \" ,y)\n",
    "lr = LinearRegression(fit_intercept=True,copy_X=True,n_jobs=2 )\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Altohugh it is not vwery useful, we can check the quality of the model on the training data.\n",
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can predict the output for any given value by using predict method:\n",
    "lr.predict([[3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is based on the linear regression, but it is useful for predicting discrete data. In other words, logistic regression is a classification algorithm rather than regression algorithm. Based on a given set of independent variables, it is used to estimate discrete value (0 or 1, yes/no, true/false).\n",
    "\n",
    "Basically, it measures the relationship between the categorical dependent variable and one or more independent variables by estimating the probability of occurrence of an event using its logistics function.\n",
    "\n",
    "<code>sklearn.linear_model.LogisticRegression</code> is the module used to implement logistic regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names =  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "target names =  ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "print(\"feature names = \", feature_names)\n",
    "print(\"target names = \" ,target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data :\n",
      "[[5.6 2.9 3.6 1.3]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.4 2.8 5.6 2.2]]\n",
      "Train labels :\n",
      "[1 1 0 1 0 2 1 2 1 2]\n",
      "Test data :\n",
      "[[5.  3.5 1.6 0.6]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]]\n",
      "Test labels :\n",
      "[0 2 0 2 2 2 0 2 0 0]\n",
      "Train size =  90\n",
      "Test size =  60\n"
     ]
    }
   ],
   "source": [
    "# creating training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data  , test_data , train_label , test_label = train_test_split(X,y, train_size=0.6 , test_size=0.4)\n",
    "print(\"Train data :\")\n",
    "print(train_data[:10])\n",
    "\n",
    "print(\"Train labels :\")\n",
    "print(train_label[:10])\n",
    "\n",
    "print(\"Test data :\")\n",
    "print(test_data[:10])\n",
    "\n",
    "print(\"Test labels :\")\n",
    "print(test_label[:10])\n",
    "\n",
    "\n",
    "print(\"Train size = \" , len(train_data))\n",
    "\n",
    "print(\"Test size = \" , len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression(n_jobs=2 , max_iter=1000)\n",
    "logr.fit(train_data,train_label)\n",
    "logr.score(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy 95% shows that logistic regression on iris dataset is quite perfect.\n",
    "logr.score(test_data, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "In ridge regression we modify the loss function by adding a penalty to make a slightly worse fit. This way, we can avoid overfiting to training data and achieve better predictions on testin data.\n",
    "\n",
    "In other words, linear regression minimizes the sum of the squared residuals, but in logistic regression we minimize the sum of the squared residuals + lambda * slope^2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('sklearn': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19cba4f2f4c3a2b2b151f1100310886641481ea2e97411d5b5f2763921367f0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
