{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is scikit-learn\n",
    "\n",
    "Scikit learn provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib.\n",
    "\n",
    "Scikit-learn includes following features:\n",
    "\n",
    "\n",
    "1. Supervised Learning algorithms − Almost all the popular supervised learning algorithms, like Linear Regression, Support Vector Machine (SVM), Decision Tree etc., are the part of scikit-learn.\n",
    "\n",
    "2. Unsupervised Learning algorithms − On the other hand, it also has all the popular unsupervised learning algorithms from clustering, factor analysis, PCA (Principal Component Analysis) to unsupervised neural networks.\n",
    "\n",
    "3. Clustering − This model is used for grouping unlabeled data.\n",
    "\n",
    "4. Cross Validation − It is used to check the accuracy of supervised models on unseen data.\n",
    "\n",
    "5. Dimensionality Reduction − It is used for reducing the number of attributes in data which can be further used for summarisation, visualisation and feature selection.\n",
    "\n",
    "6. Ensemble methods − As name suggest, it is used for combining the predictions of multiple supervised models.\n",
    "\n",
    "7. Feature extraction − It is used to extract the features from data to define the attributes in image and text data.\n",
    "\n",
    "8. Feature selection − It is used to identify useful attributes to create supervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set loading\n",
    "Scikit-learn have few example datasets like <strong>iris</strong> and <strong>digits</strong> for classification and the <strong>Boston house prices</strong> for regression.\n",
    "\n",
    "The following code shows an example of loading iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "First 10 rows of X:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "print(\"\\nFirst 10 rows of X:\\n\", X[:10])\n",
    "# As you can see, the data in the iris dataset is recorded using numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test and train sets\n",
    "We can split the dataset for training and testing. The following example will split the data into 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, trasin_label, test_label  = train_test_split(X, y , train_size=0.7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "105\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "Next, we can use our dataset to train some prediction-model. There is a wide range of Machine Learning (ML) algorithms. For example, we use KNN for classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "knn_classifier = KNeighborsClassifier(3)\n",
    "knn_classifier.fit(train_data,trasin_label)\n",
    "result = knn_classifier.predict(test_data)\n",
    "accuracy = metrics.accuracy_score(test_label , result)\n",
    "print(\"Accuracy =\" , accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence\n",
    "Once you train the model, it is desirable that the model should be persist for future use so that we do not need to retrain it again and again. It can be done with the help of dump and load features of joblib package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_classifier.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(knn_classifier, \"knn_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can load the saved model using the following method:\n",
    "loaded_classifier = joblib.load(\"knn_classifier.joblib\")\n",
    "loaded_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "Scikit-learn has package named preprocessing for this purpose. The preprocessing package has the following techniques:\n",
    "1. Binarisation\n",
    "2. Mean removal\n",
    "3. Scaling\n",
    "4. Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarisation: This preprocessing technique is used when we need to convert our numerical values into Boolean values.\n",
    "from sklearn import preprocessing\n",
    "data = [[2.1, -1.9, 5.5],\n",
    "   [-1.5, 2.4, 3.5],\n",
    "   [0.5, -7.9, 5.6],\n",
    "   [5.9, 2.3, -5.8]]\n",
    "preprocessing.binarize(data,threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean =  [ 1.75  -1.275  2.2  ]\n",
      "Std =  [2.71431391 4.20022321 4.69414529]\n",
      "[[ 0.12894603 -0.14880162  0.70300338]\n",
      " [-1.19735598  0.8749535   0.27694073]\n",
      " [-0.46052153 -1.57729713  0.72430651]\n",
      " [ 1.52893149  0.85114524 -1.70425062]]\n",
      "Mean after scale =  [1.11022302e-16 0.00000000e+00 0.00000000e+00]\n",
      "Std after scale =  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Mean removal: This technique is used to eliminate the mean from feature vector so that every feature centered on zero.\n",
    "import numpy as np\n",
    "print(\"Mean = \", np.mean(data,axis=0))\n",
    "print(\"Std = \" , np.std(data,axis=0))\n",
    "\n",
    "data_zero_centered = preprocessing.scale(data)\n",
    "\n",
    "print(data_zero_centered)\n",
    "print(\"Mean after scale = \", np.mean(data_zero_centered,axis=0))\n",
    "print(\"Std after scale = \", np.std(data_zero_centered,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02702703  0.16504854  0.98245614]\n",
      " [-1.          1.          0.63157895]\n",
      " [-0.45945946 -1.          1.        ]\n",
      " [ 1.          0.98058252 -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#  Scaling: We use this preprocessing technique for scaling the feature vectors.\n",
    "#  Scaling of feature vectors is important, because the features should not be synthetically large or small.\n",
    "min_max = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "data_scaled = min_max.fit_transform(data_zero_centered)\n",
    "print(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0230109   0.14052285  0.83646625]\n",
      " [-0.38        0.38        0.24      ]\n",
      " [-0.18681319 -0.40659341  0.40659341]\n",
      " [ 0.33550489  0.32899023 -0.33550489]]\n",
      "\n",
      "[[-0.02711951  0.16561329  0.98581782]\n",
      " [-0.64564628  0.64564628  0.4077766 ]\n",
      " [-0.30898878 -0.67250499  0.67250499]\n",
      " [ 0.58108685  0.56980361 -0.58108685]]\n"
     ]
    }
   ],
   "source": [
    "# We use this preprocessing technique for modifying the feature vectors.\n",
    "# Normalisation of feature vectors is necessary so that the feature vectors can be measured at common scale\n",
    "\n",
    "# L1 normalization: the sum of the absolute values remains always up to 1 in each row.\n",
    "l1_data = preprocessing.normalize(data_scaled,\"l1\")\n",
    "print(l1_data)\n",
    "print()\n",
    "\n",
    "# L2 normalization: the sum of the squares remains always up to 1 in each row.\n",
    "l2_data = preprocessing.normalize(data_scaled,\"l2\")\n",
    "print(l2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator API\n",
    "\n",
    "All machine learning algorithms in Scikit-Learn are implemented via Estimator API. The object that learns from the data (fitting the data) is an estimator. It can be used with any of the algorithms like classification, regression, clustering or even with a transformer, that extracts useful features from raw data.\n",
    "\n",
    "Steps in using Estimator API:\n",
    "\n",
    "Step 1: Choose a class of model\n",
    "In this first step, we need to choose a class of model. It can be done by importing the appropriate Estimator class from Scikit-learn.\n",
    "\n",
    "Step 2: Choose model hyperparameters\n",
    "In this step, we need to choose class model hyperparameters. It can be done by instantiating the class with desired values.\n",
    "\n",
    "Step 3: Arranging the data\n",
    "Next, we need to arrange the data into features matrix (X) and target vector(y).\n",
    "\n",
    "Step 4: Model Fitting\n",
    "Now, we need to fit the model to your data. It can be done by calling fit() method of the model instance.\n",
    "\n",
    "Step 5: Applying the model\n",
    "After fitting the model, we can apply it to new data. For supervised learning, use predict() method to predict the labels for unknown data. While for unsupervised learning, use predict() or transform() to infer properties of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Modeling with scikit-learn\n",
    "\n",
    "A linear model is an equation that describes a relationship between two quantities that show a constant rate of change. We represent linear relationships graphically with straight lines. A linear model is usually described by two parameters: the slope, often called the growth factor or rate of change, and the y-intercept, often called the initial value. Given the slope mm and the yy-intercept b,b, the linear model can be written as a linear function y = mx + b.y=mx+b. For example, W\n",
    "we can represent the position of a car moving at a constant velocity with a linear model.\n",
    "\n",
    "The following list shows the various linear models provided by Scikit-Learn:\n",
    "\n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Ridge Regression\n",
    "4. Bayesian Ridge Regression\n",
    "5. LASSO\n",
    "6. Multi-task LASSO\n",
    "7. Elastic-Net\n",
    "8. Multi-task Elastic-Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Linear regression is a statistical model that studies the relationship between a dependent variable (Y) with a given set of independent variables (X). \n",
    "<code>sklearn.linear_model.LinearRegression</code> is the module used to implement linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[1 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [2 3]]\n",
      "y =  [ 6  8  9 11]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression(n_jobs=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression(n_jobs=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression(n_jobs=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array([[1,1],[1,2],[2,2],[2,3]])\n",
    "print(\"X = \" , X)\n",
    "y = np.dot(X,[1,2]) + 3\n",
    "print(\"y = \" ,y)\n",
    "lr = LinearRegression(fit_intercept=True,copy_X=True,n_jobs=2 )\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Altohugh it is not vwery useful, we can check the quality of the model on the training data.\n",
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can predict the output for any given value by using predict method:\n",
    "lr.predict([[3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is based on the linear regression, but it is useful for predicting discrete data. In other words, logistic regression is a classification algorithm rather than regression algorithm. Based on a given set of independent variables, it is used to estimate discrete value (0 or 1, yes/no, true/false).\n",
    "\n",
    "Basically, it measures the relationship between the categorical dependent variable and one or more independent variables by estimating the probability of occurrence of an event using its logistics function.\n",
    "\n",
    "<code>sklearn.linear_model.LogisticRegression</code> is the module used to implement logistic regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names =  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "target names =  ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "print(\"feature names = \", feature_names)\n",
    "print(\"target names = \" ,target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data :\n",
      "[[5.7 2.8 4.1 1.3]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.4 2.9 4.3 1.3]]\n",
      "Train labels :\n",
      "[1 2 0 1 2 2 0 1 1 1]\n",
      "Test data :\n",
      "[[4.8 3.  1.4 0.1]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [5.  2.  3.5 1. ]]\n",
      "Test labels :\n",
      "[0 2 1 1 1 0 1 2 0 1]\n",
      "Train size =  90\n",
      "Test size =  60\n"
     ]
    }
   ],
   "source": [
    "# creating training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data  , test_data , train_label , test_label = train_test_split(X,y, train_size=0.6 , test_size=0.4)\n",
    "print(\"Train data :\")\n",
    "print(train_data[:10])\n",
    "\n",
    "print(\"Train labels :\")\n",
    "print(train_label[:10])\n",
    "\n",
    "print(\"Test data :\")\n",
    "print(test_data[:10])\n",
    "\n",
    "print(\"Test labels :\")\n",
    "print(test_label[:10])\n",
    "\n",
    "\n",
    "print(\"Train size = \" , len(train_data))\n",
    "\n",
    "print(\"Test size = \" , len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression(n_jobs=2 , max_iter=1000)\n",
    "logr.fit(train_data,train_label)\n",
    "logr.score(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833333333333333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy 95% shows that logistic regression on iris dataset is quite perfect.\n",
    "logr.score(test_data, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "In ridge regression we modify the loss function by adding a penalty to make a slightly worse fit. This way, we can avoid overfiting to training data and achieve better predictions on testin data.\n",
    "\n",
    "In other words, least squared regression minimizes the sum of the squared residuals, but in ridge regression we minimize the sum of the squared residuals + alpha * slope^2. \n",
    "\n",
    "The predictions in ridge regression is less sensitive tio the independent variables. For alpha = 0 the ridge regression is similar to least square regression. When we increase alpha, the slope of the regression lione decreases and it makes the regression less sensitive to the input value. Therefore, we call this alpha paramter a penalty parameter. If we increase penalty parameter, our model becomes less sensitive to the training data and it prevents overfiting and bias.\n",
    "\n",
    "<code>sklearn.linear_model.Ridge</code> is the module used to solve a regression model where loss function is the linear least squares function and regularization is L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression score 0.7949095403789805\n",
      "Ridge regression score 0.7629498741931634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "n_samples, n_features = 15, 10\n",
    "rng = np.random.RandomState(0)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "lrg = LinearRegression()\n",
    "rrg = Ridge(alpha=0.5)\n",
    "lrg.fit(X , y)\n",
    "print(\"Linear regression score\" , lrg.score(X , y))\n",
    "\n",
    "rrg.fit(X , y)\n",
    "print(\"Ridge regression score\" ,rrg.score(X , y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the score of the linear regression is higher and it shows that the linear regression is ovderfiting while the penalty paramter in the ridge regression is preveting the overfit. Now, we test a higher value of the alpha for the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7248981843700946"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrg.set_params(alpha= 1)\n",
    "rrg.fit(X,y)\n",
    "rrg.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO \n",
    "LASSO is the regularisation technique that performs L1 regularisation. It modifies the loss function by adding the penalty (shrinkage quantity) equivalent to the summation of the absolute value of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.136141297194326"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X,y)\n",
    "lasso.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial features\n",
    "Polynomial features are those features created by raising existing features to an exponent.\n",
    "\n",
    "For example, if a dataset had one input feature X, then a polynomial feature would be the addition of a new feature (column) where values were calculated by squaring the values in X, e.g. X^2. This process can be repeated for each input variable in the dataset, creating a transformed version of each. The “degree” of the polynomial is used to control the number of features added, e.g. a degree of 3 will add two new variables for each input variable. Typically a small degree is used such as 2 or 3.\n",
    "\n",
    "Generally speaking, it is unusual to use d greater than 3 or 4 because for large values of d, the polynomial curve can become overly flexible and can take on some very strange shapes. It is also common to add new variables that represent the interaction between features, e.g a new column that represents one variable multiplied by another. This too can be repeated for each input variable creating a new “interaction” variable for each pair of input variables.\n",
    "\n",
    "A squared or cubed version of an input variable will change the probability distribution, separating the small and large values, a separation that is increased with the size of the exponent. This separation can help some machine learning algorithms make better predictions and is common for regression predictive modeling tasks and generally tasks that have numerical input variables.\n",
    "\n",
    "Typically linear algorithms, such as linear regression and logistic regression, respond well to the use of polynomial input variables.\n",
    "\n",
    "scikit-learn provides a module named <strong>PolynomialFeatures</strong>. This module transforms an input data matrix into a new data matrix of given degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]\n",
      " [12 13]\n",
      " [14 15]\n",
      " [16 17]\n",
      " [18 19]\n",
      " [20 21]\n",
      " [22 23]\n",
      " [24 25]\n",
      " [26 27]\n",
      " [28 29]\n",
      " [30 31]\n",
      " [32 33]\n",
      " [34 35]]\n",
      "\n",
      "[[1.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00]\n",
      " [1.000e+00 2.000e+00 3.000e+00 4.000e+00 6.000e+00 9.000e+00]\n",
      " [1.000e+00 4.000e+00 5.000e+00 1.600e+01 2.000e+01 2.500e+01]\n",
      " [1.000e+00 6.000e+00 7.000e+00 3.600e+01 4.200e+01 4.900e+01]\n",
      " [1.000e+00 8.000e+00 9.000e+00 6.400e+01 7.200e+01 8.100e+01]\n",
      " [1.000e+00 1.000e+01 1.100e+01 1.000e+02 1.100e+02 1.210e+02]\n",
      " [1.000e+00 1.200e+01 1.300e+01 1.440e+02 1.560e+02 1.690e+02]\n",
      " [1.000e+00 1.400e+01 1.500e+01 1.960e+02 2.100e+02 2.250e+02]\n",
      " [1.000e+00 1.600e+01 1.700e+01 2.560e+02 2.720e+02 2.890e+02]\n",
      " [1.000e+00 1.800e+01 1.900e+01 3.240e+02 3.420e+02 3.610e+02]\n",
      " [1.000e+00 2.000e+01 2.100e+01 4.000e+02 4.200e+02 4.410e+02]\n",
      " [1.000e+00 2.200e+01 2.300e+01 4.840e+02 5.060e+02 5.290e+02]\n",
      " [1.000e+00 2.400e+01 2.500e+01 5.760e+02 6.000e+02 6.250e+02]\n",
      " [1.000e+00 2.600e+01 2.700e+01 6.760e+02 7.020e+02 7.290e+02]\n",
      " [1.000e+00 2.800e+01 2.900e+01 7.840e+02 8.120e+02 8.410e+02]\n",
      " [1.000e+00 3.000e+01 3.100e+01 9.000e+02 9.300e+02 9.610e+02]\n",
      " [1.000e+00 3.200e+01 3.300e+01 1.024e+03 1.056e+03 1.089e+03]\n",
      " [1.000e+00 3.400e+01 3.500e+01 1.156e+03 1.190e+03 1.225e+03]]\n"
     ]
    }
   ],
   "source": [
    "# In this example, PolynomialFeatures is used to create polynomial features of up to power of two\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "X = np.arange(36).reshape(18,2)\n",
    "print(X)\n",
    "print()\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "Xt = poly.fit_transform(X)\n",
    "print(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 100  106  120  142  172  210  256  310  372  442  520  606  700  802\n",
      "  912 1030 1156  100]\n"
     ]
    }
   ],
   "source": [
    "# Here we create a target array with one outlier to compare the performance of the logistic regression on the original dataset and the extended dataset\n",
    "y = X[:,0]*X[:,1]+100\n",
    "y[-1] = 100\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n",
      "0.8888888888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atp/Work/git/AI/scikit-learn/.sklearn/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "logr = LogisticRegression(max_iter=1000)\n",
    "logr.fit(X,y)\n",
    "print(logr.score(X,y))\n",
    "\n",
    "\n",
    "logr2 = LogisticRegression(max_iter=1000)\n",
    "logr2.fit(Xt,y)\n",
    "print(logr2.score(Xt,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimization\n",
    "\n",
    "The name Stochastic Gradient Descent - Classifier (SGD-Classifier) might mislead some user to think that SGD is a classifier. But that’s not the case! SGD Classifier is a linear classifier (SVM, logistic regression, a.o.) optimized by the SGD. These are two different concepts. While SGD is a optimization method, Logistic Regression or linear Support Vector Machine is a machine learning algorithm/model. You can think of that a machine learning model defines a loss function, and the optimization method minimizes/maximizes it.\n",
    "\n",
    "In a nutshell gradient descent is used to minimize a cost function. Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. But we can also use these kinds of algorithms to optimize our linear classifier such as Logistic Regression and linear Support Vecotor Machines.\n",
    "\n",
    "\n",
    "Scikit-learn provides <code>SGDClassifier</code> module to implement SGD classification.\n",
    "\n",
    "In the following example we use SGDClassifier with logistic regression to train a model on iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "\n",
      "[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      " 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      " 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      " 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      " 4.601e-01 1.189e-01]\n",
      "\n",
      "[ 1.09706398 -2.07333501  1.26993369  0.9843749   1.56846633  3.28351467\n",
      "  2.65287398  2.53247522  2.21751501  2.25574689  2.48973393 -0.56526506\n",
      "  2.83303087  2.48757756 -0.21400165  1.31686157  0.72402616  0.66081994\n",
      "  1.14875667  0.90708308  1.88668963 -1.35929347  2.30360062  2.00123749\n",
      "  1.30768627  2.61666502  2.10952635  2.29607613  2.75062224  1.93701461]\n",
      "\n",
      "[[0.52103744 0.0226581  0.54598853 ... 0.91202749 0.59846245 0.41886396]\n",
      " [0.64314449 0.27257355 0.61578329 ... 0.63917526 0.23358959 0.22287813]\n",
      " [0.60149557 0.3902604  0.59574321 ... 0.83505155 0.40370589 0.21343303]\n",
      " ...\n",
      " [0.45525108 0.62123774 0.44578813 ... 0.48728522 0.12872068 0.1519087 ]\n",
      " [0.64456434 0.66351031 0.66553797 ... 0.91065292 0.49714173 0.45231536]\n",
      " [0.03686876 0.50152181 0.02853984 ... 0.         0.25744136 0.10068215]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "print(breast_cancer.target_names)\n",
    "print()\n",
    "X_centered  = scale(X)\n",
    "print(X[0])\n",
    "print()\n",
    "print(X_centered[0])\n",
    "print()\n",
    "X_scaled = MinMaxScaler().fit_transform(X_centered)\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824253075571178"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_classifier = SGDClassifier(loss=\"log_loss\")\n",
    "sgd_classifier.fit(X_scaled,y)\n",
    "sgd_classifier.score(X_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9718804920913884"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "linear_classifier = LogisticRegression()\n",
    "linear_classifier.fit(X_scaled , y)\n",
    "linear_classifier.score(X_scaled , y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machine\n",
    "\n",
    "A support vector machine (SVM) is a type of deep learning algorithm that performs supervised learning for classification or regression of data groups. It draws lines (hyperplanes) to separate the groups according to patterns.\n",
    "\n",
    "Like other supervised learning machines, an SVM requires labeled data to be trained. Groups of materials are labeled for classification. Training materials for SVMs are classified separately in different points in space and organized into clearly separated groups. \n",
    "\n",
    "According to the SVM algorithm we find the points closest to the line from both the classes. These points are called support vectors. Now, we compute the distance between the line and the support vectors. This distance is called the margin. Our goal is to maximize the margin. The hyperplane for which the margin is maximum is the optimal hyperplane.\n",
    "\n",
    "\n",
    "There are two imporant parameters that you can pass to change the way this hyperplane is created:\n",
    "\n",
    "<h4>C:</h4>\n",
    "It controls the trade off between smooth decision boundary and classifying training points correctly. A large value of c means you will get more training points correctly.\n",
    "\n",
    "\n",
    "<strong>Smooth decision boundary vs classifying all points correctly</strong>\n",
    "\n",
    "Consider an example as shown in the figure above. There are a number of decision boundaries that we can draw for this dataset. Consider a straight (green colored) decision boundary which is quite simple but it comes at the cost of a few points being misclassified. These misclassified points are called outliers. We can also make something that is considerably more wiggly(sky blue colored decision boundary) but where we get potentially all of the training points correct. Of course the trade off having something that is very intricate, very complicated like this is that chances are it is not going to generalize quite as well to our test set. So something that is simple, more straight maybe actually the better choice if you look at the accuracy. Large value of c means you will get more intricate decision curves trying to fit in all the points. Figuring out how much you want to have a smooth decision boundary vs one that gets things correct is part of artistry of machine learning. So try different values of c for your dataset to get the perfectly balanced curve and avoid over fitting.\n",
    "\n",
    "<h4>Gamma:</h4>\n",
    "\n",
    "It defines how far the influence of a single training example reaches. If it has a low value it means that every point has a far reach and conversely high value of gamma means that every point has close reach.\n",
    "\n",
    "If gamma has a very high value, then the decision boundary is just going to be dependent upon the points that are very close to the line which effectively results in ignoring some of the points that are very far from the decision boundary. This is because the closer points get more weight and it results in a wiggly curve as shown in previous graph.On the other hand, if the gamma value is low even the far away points get considerable weight and we get a more linear curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226713532513181"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(X,y)\n",
    "svc.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9279437609841827"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=2)\n",
    "svc.fit(X,y)\n",
    "svc.score(X,y)\n",
    "\n",
    "# Larger value of C results in a larger score on training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.sklearn': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94ac2878702d187c1d29c60bcefe76b42a44db078259ef57bc719f99e507b10d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
